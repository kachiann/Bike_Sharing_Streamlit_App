{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in /Users/kachiemenike/Documents/blue_yonder_bike_sharing/.venv/lib/python3.12/site-packages (0.0.3)\n"
     ]
    }
   ],
   "source": [
    "# Install the ucimlrepo package to easily import dataset\n",
    "!pip install ucimlrepo\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Builing a baseline model\n",
    "\n",
    "We first build a baseline model which serves as a starting point for comparison with more complex models. That is, we can use this baseline as a benchmark to compare the performance of more advanced models developed later.\n",
    "\n",
    "### Building a linear model\n",
    "\n",
    "We first develop a linear regression model aimed at predicting the total count of rental bikes, encompassing both casual and registered users, by leveraging a collection of input variables. This model undergoes training utilizing the training dataset, wherein the input variables are employed to anticipate the target variable (`cnt`).\n",
    "\n",
    "### Building tree-based models and selecting the best\n",
    "\n",
    "Next, we develop tree-based models, which  involves constructing decision trees or ensemble methods such as Random Forests or Gradient Boosting Trees.\n",
    "\n",
    "First, let's check the generalization performance of decision tree regressor and the ensemble methods with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Regressor - Mean Absolute Error (MSE) on training set: 143.26\n",
      "Dummy Regressor - Mean Absolute Error (MAE) using cross-validation: 143.27 ± 3.30\n",
      "\n",
      "Linear Regressor - Mean Absolute Error (MSE) on training set: 106.55\n",
      "Linear Regressor - Mean Absolute Error (MAE) using cross-validation: 106.63 ± 2.72\n",
      "\n",
      "Decision Tree Regressor - Mean Absolute Error (MSE) on training set: 0.02\n",
      "Decision Tree Regressor - Mean Absolute Error (MAE) using cross-validation: 35.58 ± 1.16\n",
      "\n",
      "Random Forest Regressor - Mean Absolute Error (MSE) on training set: 9.60\n",
      "Random Forest Regressor - Mean Absolute Error (MAE) using cross-validation: 26.18 ± 0.73\n",
      "\n",
      "Gradient Boosting Regressor - Mean Absolute Error (MSE) on training set: 48.74\n",
      "Gradient Boosting Regressor - Mean Absolute Error (MAE) using cross-validation: 48.92 ± 1.54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "hourly_dataset  = fetch_ucirepo(id=275)\n",
    "features = hourly_dataset.data.features\n",
    "features = features.drop(['dteday'], axis=1) \n",
    "target = hourly_dataset.data.targets\n",
    "target = target['cnt'].values\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Execute the train.py script\n",
    "%run train_and_evaluate_model.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the linear regression model shows consistent performance in predicting the count of rental bikes, with a mean absolute error of around 106.55-106.63 across both the training set and cross-validation. This suggests that the model captures the underlying patterns in the data reasonably well, though there may still be room for improvement in reducing prediction errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Decision Tree Regressor: The mean absolute error (MAE) on the training set is extremely low, indicating a near-perfect fit to the training data, which might suggest overfitting. Also, the average MAE using cross-validation is higher, indicating that the model's performance may not generalize well to unseen data.\n",
    "- Random Forest Regression: The MAE on the training set is higher compared to the Decision Tree Regressor but still relatively low. The average MAE using cross-validation is lower compared to the Decision Tree Regressor, suggesting better generalization performance.\n",
    "- Gradient Boosting Regression: The MAE on the training set is noticeably higher compared to both Decision Tree Regressor and Random Forest Regression, suggesting less overfitting. The average MAE using cross-validation is even higher, reinforcing the concern about overfitting and poor generalization.\n",
    "\n",
    "Among the models evaluated, the Random Forest Regression model appears to perform the best. It achieves a relatively low average MAE on the training set and demonstrates better generalization performance compared to the Decision Tree and Gradient Boosting Regression models.\n",
    "\n",
    "However, further investigation is warranted to optimize the Random Forest model's hyperparameters and address potential overfitting. Hyperparameter tuning and feature engineering could potentially enhance the model's performance and make it more robust for real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning - Linear regression model -Ridge or Lasso\n",
    "- This focuses on tuning the regularization strength parameter (alpha) for both Lasso and Ridge regression using cross-validation and grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression:\n",
      "Best alpha: {'alpha': 0.01}\n",
      "Training MSE: 106.55\n",
      "\n",
      "Ridge Regression:\n",
      "Best alpha: {'alpha': 1}\n",
      "Training MSE: 106.55\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 0.5, 1, 10, 100]}\n",
    "\n",
    "# Lasso Regression\n",
    "lasso = Lasso()\n",
    "lasso_cv = GridSearchCV(lasso, param_grid, cv=10)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "# Compute predictions on the training set\n",
    "y_train_pred_lasso= lasso_cv.predict(X_train)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge = Ridge()\n",
    "ridge_cv = GridSearchCV(ridge, param_grid, cv=10)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "y_train_pred_ridge= ridge_cv.predict(X_train)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the models\n",
    "print(\"Lasso Regression:\")\n",
    "print(\"Best alpha:\", lasso_cv.best_params_)\n",
    "print(f\"Training MSE: {mean_absolute_error(y_train, y_train_pred_lasso):.2f}\")\n",
    "\n",
    "\n",
    "print(\"\\nRidge Regression:\")\n",
    "print(\"Best alpha:\", ridge_cv.best_params_)\n",
    "print(f\"Training MSE: {mean_absolute_error(y_train, y_train_pred_ridge):.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No much improvement after tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning - Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Cross-validated score of the best estimator: 26.06\n",
      "Mean Absolute Error on Training Set: 9.46\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the Random Forest Regression model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with the model, hyperparameter grid, and scoring metric\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,\n",
    "                           cv=10, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and best mean absolute error\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(f\"Cross-validated score of the best estimator: {-grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Evaluate the model with best hyperparameters on the training set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "print(f\"Mean Absolute Error on Training Set: {train_mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there were no much improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Models\n",
    "| Model Name | Mean Absolute Error (MAE) on training set | MAE via cross-validation |\n",
    "|---------------|---------------|---------------|\n",
    "| 1. Dummy Regressor (Baseline) | 143.26 | |\n",
    "| 2. Linear Regression | 106.554  | 106.630 ± 2.722 |\n",
    "| 3. Decision Tree | 0.015 | 35.621 ± 1.119 |\n",
    "| 4. Random Forest | 9.604  | 26.213 ± 0.640  |\n",
    "| 5. Random Forest (after tuning) | 9.459 | 26.063 |\n",
    "| 6. Gradient Boosting | 48.744 | 48.916 ± 1.537 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
